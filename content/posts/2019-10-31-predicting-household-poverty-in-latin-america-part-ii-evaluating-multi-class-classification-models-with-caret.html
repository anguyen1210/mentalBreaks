---
title: 'Predicting household poverty in Latin America, Part II: Evaluating multi-class
  classification models with caret'
author: Anthony Nguyen
date: '2019-10-31'
slug: predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret
categories:
   - r-bloggers
tags:
  - poverty
  - machine-learning
  - supervised-learning
  - classification
  - r-caret
lastmod: '2020-01-03T07:32:24+01:00'
layout: post
type: post
highlight: no
---



<p>In this post, I evaluate the performance of some popular supervised classification algorithms using <code>caret</code> and the <a href="https://www.kaggle.com/c/costa-rican-household-poverty-prediction/overview">Costa Rican Household Poverty (CRHP) dataset</a> provided by the Inter-American Development Bank (IDB), by way of Kaggle.</p>
<div id="the-challenge-and-the-data" class="section level2">
<h2>The challenge and the data</h2>
<p>In Latin America, as in many other parts of the world, accurate targeting of social welfare programs is made difficult given the lack of income and expense records in the poorest segments of the population. To overcome this obstacle, Proxy Means Tests (PMT) are often used to identify those who would qualify for social assistance based on observable household attributes. Hoping to improve existing PMT’s, the IDB sponsored a competition on Kaggle that aimed to identify new methods for more accurately identifying those most in need of social assistance.</p>
</div>
<div id="exploratory-analysis" class="section level2">
<h2>Exploratory analysis</h2>
<p>In <a href="http://127.0.0.1:4321/posts/predicting-household-poverty-in-latin-america-evaluating-multi-class-classification-models-in-caret/">Part I of this post</a>, I re-split, cleaned and pre-processed the original data from Kaggle in order for us to be able conduct our own analysis. In this second part, I will conduct some basic exploratory analysis to see if we can learn anything about our data that will help us with building or model. As these two posts are really aimed at beginners, I display and annotate as much code as possible. To get started, we will make use of the following packages:</p>
<pre class="r"><code>library(tidyverse) 
library(caret)
library(gridExtra)
library(kableExtra)
library(rpart.plot)</code></pre>
<div id="distribution-of-poverty-levels" class="section level3">
<h3>Distribution of poverty levels</h3>
<p>This is a question we already looked at a bit when we partitioned our data, but it seems a natural starting point for our exploratory analysis, so we’ll run through it quickly again. First, we can have a look at the distribution of poverty levels in our filtered training set:</p>
<pre class="r"><code>train_filtered %&gt;% 
  ggplot(aes(as.numeric(Target))) +
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = &quot;count&quot;, show.legend = TRUE) +
  geom_text(aes(label = scales::percent(..prop..), y = ..prop..), stat = &quot;count&quot;, vjust = -.5, size = 3) +
  scale_y_continuous(labels = scales::percent) +
  ylab(&quot;relative frequencies&quot;) +
  ggtitle(&quot;Relative frequency of poverty levels, filtered training set&quot;) +
  theme_minimal() +
  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.title.x = element_blank()) + 
  scale_fill_viridis_d(name=&quot;Poverty level (Target)&quot;, 
                      breaks=c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;), 
                      labels=c(&quot;1 = extreme poverty&quot;, &quot;2 = moderate poverty&quot;, &quot;3 = vulnerable&quot;, &quot;4 = non-vulnerable&quot;))</code></pre>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/p_target_train.png" /></p>
<p>The first thing we notice is that we have an imbalanced set of classification targets. Those labeled as belonging to the extreme poverty group only show up 8.1% of the time, while those who are not vulnerable to falling into poverty are prevalent 62.8% of the time. We will keep this in mind later when choosing our model training and evaluation metrics.</p>
</div>
<div id="distribution-of-other-numeric-variables" class="section level3">
<h3>Distribution of other numeric variables</h3>
<p>When we imported our data into <code>R</code> all numeric variables were converted to the class <dbl>. Unfortunately, this doesn’t tell us which ones are binary dummy variables, and which ones take on other values greater than 1. First we take a subset all of the non-dummy variables and have a look to see if there is any interesting information here:</p>
<pre class="r"><code># identify any columns that take on values more than 0:1
no_dummy_index  &lt;- apply(train_filtered, 2, function(x) { !all(x %in% 0:1) }) 

# subset out the no dummy columns
no_dummy &lt;- train_filtered[ , no_dummy_index] 

# print names of no dummy columns
names(no_dummy)</code></pre>
<pre><code>##  [1] &quot;Id&quot;           &quot;idhogar&quot;      &quot;rooms&quot;        &quot;r4h1&quot;        
##  [5] &quot;r4h2&quot;         &quot;r4h3&quot;         &quot;r4m1&quot;         &quot;r4m2&quot;        
##  [9] &quot;r4m3&quot;         &quot;r4t1&quot;         &quot;r4t2&quot;         &quot;r4t3&quot;        
## [13] &quot;tamhog&quot;       &quot;tamviv&quot;       &quot;escolari&quot;     &quot;hogar_nin&quot;   
## [17] &quot;hogar_adul&quot;   &quot;hogar_mayor&quot;  &quot;edjefe&quot;       &quot;edjefa&quot;      
## [21] &quot;bedrooms&quot;     &quot;overcrowding&quot; &quot;qmobilephone&quot; &quot;age&quot;         
## [25] &quot;Target&quot;</code></pre>
<p>We notice a few things here: besides our two ID variables, which we can ignore, there’s a mix of individual and household-level variables. After consulting the descriptions for all of these, I’ll just examine a few that may be of more interest:</p>
<blockquote>
<ul>
<li><code>age</code>, Age in years</li>
<li><code>escolari</code>, years of schooling</li>
<li><code>qmobilephone</code>, # of mobile phones</li>
<li><code>tamviv</code>, number of persons living in the household</li>
<li><code>overcrowding</code>, # persons per room</li>
<li><code>hogar_nin</code>, Number of children 0 to 19 in household</li>
</ul>
</blockquote>
<p>We can have a look at how these variable are distributed according to the poverty-level of the indvidual in the plots below:</p>
<pre class="r"><code>#define function to plot feature variable against target 
plot_target_by_var &lt;- function(df, variable){
    var &lt;- enquo(variable)
    df %&gt;% 
  ggplot(aes(!!var, fill = as.factor(Target), color = as.factor(Target))) + 
  geom_density(alpha = 0.1, size = 1, show.legend = FALSE) + 
  scale_colour_viridis_d() +
  theme_minimal()
}

#plot variables of interest
p_age &lt;- plot_target_by_var(train_filtered, age)
p_escolari &lt;- plot_target_by_var(train_filtered, escolari)
p_qmobilephone &lt;- plot_target_by_var(train_filtered, p_qmobilephone)
p_tamviv &lt;- plot_target_by_var(train_filtered, tamviv)
p_overcrowding &lt;- plot_target_by_var(train_filtered, overcrowding)
p_hogar_nin &lt;- plot_target_by_var(train_filtered, hogar_nin)

#display plots in a grid
grid.arrange(p_age, p_escolar, p_qmobilephone, p_tamviv, p_overcrowding, p_hogar_nin, nrow = 2, top = textGrob(&quot;Distribution of selected features by `Target` class&quot;, gp=gpar(fontsize=15)))</code></pre>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/p_target_selected_features.png" /></p>
<p>From these density plots, we can see that those labeled as being in the most extreme poverty (purple line) are likely to be much younger, and have less education than the other groups (which would go together, naturally). With regards to education (<code>escolari</code>), we can also see that those who are labeled as not vulnerable to poverty (yellow line) are much more likely than the other groups to have greater than 12 or so years of education. Furthermore, we can see that there is a slightly higher tendency for extremely and moderately poor households to have more people living in the house (<code>tamviv</code>) in the 7+ persons per household range, and similarly, they tend to have more <code>overcrowding</code> in terms of the 2-4 persons per room range.</p>
</div>
<div id="correlation-to-target-variable" class="section level3">
<h3>Correlation to <code>Target</code> variable</h3>
<p>Another thing we can look at before we start building up our model is the correlation between the <code>Target</code> poverty levels with all of the features we’ve retained so far in our dataset.</p>
<pre><code>##    escolari     epared3  pisomoscer   cielorazo       rooms paredblolad 
##   0.3220620   0.2956923   0.2850161   0.2746833   0.2641804   0.2532956 
##  instlevel8      edjefe    bedrooms        v18q 
##   0.2341791   0.2298932   0.2159925   0.2069992</code></pre>
<p>The list above are the top 10 variables that correlate most strongly with our <code>Target</code>variable in either direction. In addition to some of the variables we just looked at, we see a few of the binary dummy variables high up in this list, including <code>epared3</code> which indicates if the walls of the home are good, <code>pisomoscer</code> indicating if the floors are made of ceramic, <code>paredblolad</code> indicating if the home walls are brick, and <code>cielorazo</code> indicating whether or not the house has a ceiling.</p>
</div>
</div>
<div id="modeling" class="section level2">
<h2>Modeling</h2>
<p>While the original Kaggle competition had participants only focusing on making their predictions for the heads of each household (<code>parentesco</code>= 1), to make our job simpler in this analysis, we will just be making a prediction for all individuals in the dataset.</p>
<p>Our approach will be to run a small number of different machine learning models on our training data using just the default settings to get and idea of how they each perform. After our initial run through, we could then pick a few models and see if we can better tune them and improve our predictions.</p>
<p>Just before we start, we can drop both of our ID variables (<code>Id</code>, <code>idhogar</code>). Additionally, we need to transform or <code>Target</code> variable into a factor at this stage in order to ensure our machine learning models treat this problem as a classification task.</p>
<pre class="r"><code># drop ID variables from both `train_filtered` and `test_filtered`
train_filtered &lt;- train_filtered[ ,3:78]
test_filtered &lt;- test_filtered[ ,3:78]

# transfrom `Target` into factor for both sets
train_filtered$Target &lt;- as.factor(train_filtered$Target)
test_filtered$Target &lt;- as.factor(test_filtered$Target)</code></pre>
<div id="fitting-different-classification-models" class="section level3">
<h3>Fitting different classification models</h3>
<p>For this first part of our modelling exercise, we will just try out a few of the most well-known classification algorithms that are covered in the <code>caret</code> package. As we are dealing with imbalanced data, the default parameter we will try to optimize during training will be <a href="https://topepo.github.io/caret/model-training-and-tuning.html#alternate-performance-metrics"><code>Kappa</code> in order to boost our performance</a>. This can be done by adding the <code>metric = "Kappa"</code> argument to our <code>train()</code>call.</p>
<p>For performance and speed improvements, we will use a 10 K-fold cross validation to fit our models, which we can implement using the <code>trControl</code> function in <code>caret</code>.</p>
<p>We can fit all of our initial models on the training data with the following code:</p>
<pre class="r"><code># define models to try
models &lt;- c(&quot;multinom&quot;, &quot;lda&quot;, &quot;naive_bayes&quot;, &quot;svmLinear&quot;, &quot;knn&quot;, &quot;rpart&quot;, &quot;ranger&quot;)  

# set CV control for knn, k-folds
control &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = .9) # 10 fold, 10%

# fit models
set.seed(1)

train_models &lt;- lapply(models, function(model){ 
    print(model)
    train(Target ~ ., method = model, data = train_filtered, trControl = control, metric = &quot;Kappa&quot;) 
}) 
    
names(train_models) &lt;- models</code></pre>
</div>
</div>
<div id="evaluation-macro-scores" class="section level2">
<h2>Evaluation: Macro scores</h2>
<p>It is important to choose the right metric for evaluating the model. Knowing the we are performing a supervised multi-class classification task on unbalanced data, we should look at alternatives to the standard overall <em>accuracy</em> measure. Specifically, we need to account for issues of prevalence, as people belonging to the class labeled as <em>extreme poverty</em> make up only <strong>8%</strong> of our data, meaning that if our model fails to predict these people correctly (low <em>sensitivity</em>) it will not lower our <em>accuracy</em> much. In order to take into account the balance between <em>sensitivity</em> and <em>specificity</em>, for this project, we will use the <a href="https://rafalab.github.io/dsbook/introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score"><span class="math inline">\(F_1\)</span> score</a>, which is used to give a one number summary that is the harmonic average of <em>precision</em> and <em>recall</em>.</p>
<p>The standard <span class="math inline">\(F_1\)</span> score for all of our class predictions can be calculated as such:
<span class="math display">\[
F_1 = 2 \times \frac{\mbox{precision} \cdot \mbox{recall}}
{\mbox{precision} + \mbox{recall}}
\]</span>
It can also be useful for us to look at a one number summary of this <span class="math inline">\(F_1\)</span> score, so we will compute both the Macro-<span class="math inline">\(F_1\)</span> and the Weighted-<span class="math inline">\(F_1\)</span>, each of which try to balance the importance of the different classes in our dataset. (Follow the link for a nice <a href="https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1">summary of Macro-<span class="math inline">\(F_1\)</span> and Weighted-<span class="math inline">\(F_1\)</span> scores</a>.</p>
<p>To calculate the Macro-<span class="math inline">\(F_1\)</span>, we just take the average of the <span class="math inline">\(F_1\)</span> scores for all our classes (<span class="math inline">\(1,...,k\)</span>) , so that:
<span class="math display">\[
\text{Macro}\,F_1 = \frac{1}{k} \sum_{i=1}^{k} 2 \times \frac{\mbox{precision}_i \cdot \mbox{recall}_i}
{\mbox{precision}_i + \mbox{recall}_i}
\]</span></p>
<p>For the Weighted-<span class="math inline">\(F_1\)</span>, we perform a similar calculation, only that we weight the <span class="math inline">\(F_1\)</span> score by the number of samples from that class. So, for each of our clusters <span class="math inline">\(1,...,k\)</span>, where <span class="math inline">\(k\)</span> has an actual number of <span class="math inline">\(n\)</span> in our sample, we can compute the weighted average so that:
<span class="math display">\[
\text{Weighted}\,F_1 = \frac{n_i  \sum_{i=1}^{k} 2 \times \frac{\mbox{precision}_i \cdot \mbox{recall}_i}
{\mbox{precision}_i + \mbox{recall}_i}}{\sum_{i=1}^{k}n_i}
\]</span></p>
<p>The Macro-<span class="math inline">\(F_1\)</span> gives equal weights to all classes, so it should give extra emphasis to our under-represented classes, whereas the Weighted-<span class="math inline">\(F_1\)</span> gives emphasis proportional to the size of each class. We can extract the relevant evaluation metrics from our list of fitted models using <code>caret</code> and the following code:</p>
<pre class="r"><code># extract elapsed training times
elapsed &lt;- sapply(train_models, function(object) 
    object$times$everything[&quot;elapsed&quot;])

# extract accuracy from CM in one step without creating a separate predictions vector
acc = sapply(train_models, function(x){
    pred = predict(x, newdata = test_filtered)
    cm = confusionMatrix(pred, reference = test_filtered$Target)
    return(cm[[&quot;overall&quot;]][&quot;Accuracy&quot;])
  }
)

# extract F1 by class
F1 = sapply(train_models, function(x){
    pred = predict(x, newdata = test_filtered)
    cm = confusionMatrix(pred, reference = test_filtered$Target)
    return(cm[[&quot;byClass&quot;]][ , &quot;F1&quot;])
  }
)

# extract macro F1
F1_M = sapply(train_models, function(x){
    pred = predict(x, newdata = test_filtered)
    cm = confusionMatrix(pred, reference = test_filtered$Target)
    return(mean(cm[[&quot;byClass&quot;]][ , &quot;F1&quot;], na.rm = TRUE))
  }
)

# extract weighted F1
F1_W &lt;- sapply(train_models, function(x){
    pred = predict(x, newdata = test_filtered)
    cm = confusionMatrix(pred, reference = test_filtered$Target)
    actual = colSums(cm$table)
    F1 = cm[[&quot;byClass&quot;]][ , &quot;F1&quot;]
    return((sum(actual*F1, na.rm = TRUE))/(sum(actual)))
  }
)</code></pre>
<p>Here we compile our evaluation metrics into a table for comparison:</p>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/resultstable1.png" /></p>
<p>A few things jump out immediately from this summary: (1) the range of speed at which these models run is quite varied. On one end, <em>linear discriminant analysis (LDA)</em> (<code>lda</code>), <em>naive bayes</em> (<code>naive_bayes</code>), <em>decision tree</em> (<code>rpart</code>) were extremely fast and only took a matter of seconds to compute; while at the other end, <em>multinomial logistic regression</em> (<code>multinom</code>), <em>support vector machine (SVM)</em> (<code>svmLinear</code>), and <em>random forest</em> (<code>ranger</code>) took 30 to 100 times longer to run. Our <em>K-nearest neighbor</em> (<code>knn</code>) is in the middle, and ran relatively fast. With regards to how these model fitting times relate to performance, we can go through each of our evaluation metrics one-by-one.</p>
<div id="speed-vs.-accuracy" class="section level3">
<h3>Speed vs. Accuracy</h3>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/p_speed_acc.png" /></p>
<p>Looking at the overall accuracy scores we can see that <em>multinomial logistic regression</em> outperforms that other models, though the computational time is towards the long end. For the fastest running models, <em>LDA</em> gives the best accuracy.</p>
</div>
<div id="speed-vs.-macro-f1-score" class="section level3">
<h3>Speed vs. Macro-F1 Score</h3>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/p_speed_f1m.png" /></p>
<p>When we look at the Macro-F1 scores, the picture changes quite a bit. The models seem to stay in their same speed groupings, but the best performing model is now our <em>decision tree</em> (<code>rpart</code>), with the <em>SVM</em>, coming in second-place. Given the gap between our implementation of <code>rpart</code>and the rest, I’m inclined to think there was a problem with this Macro-F1 calculation, so we will table this point for further exploration later.</p>
</div>
<div id="speed-vs.-weighted-f1-score" class="section level3">
<h3>Speed vs. Weighted-F1 Score</h3>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/p_speed_f1w.png" /></p>
<p>Much like when using our accuracy metric, our implementation of <code>multinom</code> had the best performance here, followed by <code>lda</code>.</p>
</div>
</div>
<div id="evaluation-micro-scores" class="section level2">
<h2>Evaluation: Micro scores</h2>
<p>We’ve seen above that there our macro-averaged metrics seemingly indicate that <code>multinom</code> is giving us the best performance, though at a cost of moderately long computational time, while much more quickly, <code>lda</code>, <code>rpart</code>, or even <code>knn</code> are doing reasonably well using only a fraction of the same time.</p>
<p>Given that our prediction task is focused on an imbalanced class set, and also given that the most underrepresented class (i.e. “extreme poverty”) is, in fact, the one that we are most interested in, it might be worthwhile to have a look at some the F1 metrics for each of these class predictions to see which models are doing the best.</p>
<div id="f_1-score-by-class" class="section level3">
<h3><span class="math inline">\(F_1\)</span>-score by class</h3>
<p>First we can have a look at a summary table of the <span class="math inline">\(F_1\)</span>-scores we computed for each class over all of the models:</p>
<pre class="r"><code>F1 %&gt;% kable() %&gt;% kable_styling()</code></pre>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/f1.png" /></p>
<p>Immediately we can spot some problems–namely, the NaN and NA values in <code>svmLinear</code>and <code>rpart</code>. This is certainly distorting our calculation of the Macro-<span class="math inline">\(F_1\)</span> and Weighted-<span class="math inline">\(F_1\)</span> scores for those two models, which should be kept in mind.</p>
<p>For <code>Class: 1</code>, the “extreme poverty” category, we can see that <code>naive_bayes</code>, is, in fact, performing the best.</p>
<pre class="r"><code>which.max(F1[1, ])</code></pre>
</div>
<div id="f-beta-score-by-class" class="section level3">
<h3><span class="math inline">\(F-\beta\)</span> score by class</h3>
<p>While the <span class="math inline">\(F_1\)</span> score accounts for both <em>sensitivity</em> and <em>specificity</em> (i.e. <em>recall</em>). The <span class="math inline">\(F_1\)</span> score weighs the importance of these two metrics equally, but we can add weights to emphasize which of the two are more important to us. For example, in this project, as we are most interested in the least-represented class in the dataset (<code>Class: 1</code>, “extreme poverty”), we can use different <span class="math inline">\(F_\beta\)</span> scores to give greater importance to <em>recall</em> our calculation of the standard <span class="math inline">\(F_1\)</span> metric. We calculate the <span class="math inline">\(F_\beta\)</span> score with:
<span class="math display">\[
F_\beta = (1 + \beta^2) \times \frac{\mbox{precision} \cdot \mbox{recall}}
{(\beta^2 \cdot \mbox{precision}) + \mbox{recall}}
\]</span></p>
<p>For this measure, the <span class="math inline">\(\beta\)</span> takes on a number between 1 and <span class="math inline">\(\infty\)</span>, where a value of 1, simply just returns the <span class="math inline">\(F_1\)</span> score. Just to see how our models react, I will compute an <span class="math inline">\(F_2\)</span> and <span class="math inline">\(F_3\)</span> score and evaluate.</p>
<pre class="r"><code>F2 &lt;- sapply(train_models, function(x){
    pred = predict(x, newdata = test_filtered)
    cm = confusionMatrix(pred, reference = test_filtered$Target)
    precision = cm[[&quot;byClass&quot;]][ , &quot;Precision&quot;]
    recall = cm[[&quot;byClass&quot;]][ , &quot;Recall&quot;]
    return((5*precision*recall) / (4*precision + recall))
  }
)

# extract F-Beta by class
F3 &lt;- sapply(train_models, function(x){
    pred = predict(x, newdata = test_filtered)
    cm = confusionMatrix(pred, reference = test_filtered$Target)
    precision = cm[[&quot;byClass&quot;]][ , &quot;Precision&quot;]
    recall = cm[[&quot;byClass&quot;]][ , &quot;Recall&quot;]
    return((10*precision*recall) / (9*precision + recall))
  }
)</code></pre>
<div id="class-1-extreme-poverty" class="section level4">
<h4>Class 1: Extreme Poverty</h4>
<pre class="r"><code>class1 &lt;- rbind(F1[1, ], F2[1, ], F3[1, ])
row.names(class1) &lt;-  c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;)
class1 %&gt;% kable(caption = &quot;F-Beta Scores, Class: 1, Extreme Poverty&quot;) %&gt;% kable_styling()</code></pre>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/fbeta_class1.png" /></p>
<p>In the table above, we can see how the different <span class="math inline">\(F_beta\)</span> scores affect our <code>Class: 1</code> performance as we give increasing weight to <em>recall</em> in our evaluation. The most striking finding here is that as the <span class="math inline">\(\beta\)</span> value increases, the <span class="math inline">\(F_\beta\)</span> values for all of the models decrease, <em>except</em> for <code>naive_bayes</code>, which actually starts to perform better.</p>
</div>
<div id="class-4-non-vulnerable" class="section level4">
<h4>Class 4: Non-vulnerable</h4>
<pre class="r"><code>class4 &lt;- rbind(F1[4, ], F2[4, ], F3[4, ])
row.names(class4) &lt;-  c(&quot;F1&quot;, &quot;F2&quot;, &quot;F3&quot;)
class4 %&gt;% kable(caption = &quot;F-Beta Scores, Class: 4, Non-vulnerable&quot;) %&gt;% kable_styling()</code></pre>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/fbeta_class4.png" /></p>
<p>Looking at what happens to predictions at the top end of our class distribution, we find that as the <span class="math inline">\(\beta\)</span> value increases, all of our models start to perform better <em>except</em> for <code>naive_bayes</code> which actually begins to perform worse.</p>
</div>
</div>
</div>
<div id="feature-importance" class="section level1">
<h1>Feature importance</h1>
<p>One final thing that would be interesting to look at is the relative importance of the features used to make the predictions. With <code>rpart</code>, we can extract this information relatively easily. As we noted above, there were a few models that returned <span class="math inline">\(F_1\)</span> scores of NA, and <code>rpart</code> was one of them. This can happen with a a simple decision tree classifier, as sometimes, there are not enough observations of a particular class to make a difference to the error measurement of the classifier so the class just gets dropped. This issue could potentially be fixed by using <em>upsampling</em> or <em>downsampling</em> techniques, but for the moment, let’s just have a look at the final model:</p>
<pre class="r"><code>#plot decision tree
rpart.plot::rpart.plot(train_models$rpart$finalModel)</code></pre>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/rpart.png" /></p>
<p>This, of course, is an incomplete picture as the <code>Class: 1</code> and <code>Class: 3</code> labels were dropped from this model, but in terms of the most important features used to calculate the overall accuracy, we can get an idea of what variables were considered the most important. The thing that stands out the most here is that, apart from <code>v18q</code>, which is a dummy variable capturing whether or not an individual owns a tablet, all other splitting nodes are ones that capture information specific to the household, rather than the individual.</p>
<p>More directly, we can call up a list of the most important features using the <code>varImp</code> function from <code>caret</code>. Among all of the models we have tried to fit so far, the only other model that accepts this function is <code>multinom</code>, so we will call this one up as well to compare. Below is the top 10 most important variables for our decision tree and multinomial logit model:</p>
<pre class="r"><code>get_imp &lt;- function(modelname){
    imp &lt;- data.frame(varImp(modelname)$importance)
    imp$Variable &lt;- rownames(imp)
    imp &lt;- imp[order(-imp$Overall)[1:5], ] %&gt;% select(Variable, Overall) 
    rownames(imp) &lt;- 1:5
    return(imp)
}

imp_multinom &lt;- get_imp(train_models$multinom)
imp_rpart &lt;- get_imp(train_models$rpart)

get_imp &lt;- function(modelname){
    imp &lt;- data.frame(varImp(modelname)$importance)
    imp$Variable &lt;- rownames(imp)
    imp &lt;- imp[order(-imp$Overall)[1:10], ] %&gt;% select(Variable, Overall) 
    rownames(imp) &lt;- 1:10
    return(imp)
}

cbind(imp_rpart, imp_multinom) %&gt;% kable(caption = &quot;Top 10 important variables, `rpart` vs. `multinom`&quot;) %&gt;% kable_styling()</code></pre>
<p><img src="/posts/2019-10-31-predicting-household-poverty-in-latin-america-part-ii-evaluating-multi-class-classification-models-with-caret_files/imp_vars.png" /></p>
<p>With regards to the top-10 most important variables from the <code>rpart</code> list, we’ve seen most of these names already in the decision tree and in the list of the 10 most correlated variables with out <code>Target</code> variable list that we showed earlier. More interestingly, the list of the ten most important <code>multinom</code> variables is completely different than anything we’ve seen so far, which s is also interesting as this was identified as one of the highest performing models among the ones we’ve tested.</p>
<div id="some-concluding-remarks" class="section level2">
<h2>Some concluding remarks</h2>
<p>Using our re-split CRHP data, were were able to build and test a few of the most widely known classification models to predict the relative poverty levels for individuals in Costa Rica. We trained seven models in total: <em>multinomial logistic regression</em>, <em>linear discriminant analysis (LDA)</em>, <em>naive bayes</em>, <em>support vector machine (SVM)</em>, <em>K-nearest neighbors</em>, <em>decision tree</em>, and <em>random forest</em>.</p>
<p>We computed a few different evaluation metrics to analyze the performance of our models, taking into account there overall average performance in predicting among all classes, as well as their performance predicting in the class we are most interested in, the extreme poor. Depending on what we emphasized as most important, different models performed better or worse than the others. For overall predictive performance, <code>multinom</code> and <code>lda</code> appeared to do the best, though time it takes to fit the <code>lda</code> model is a fraction of the time it takes to run the <code>multinom</code> model. When turning our attention to predictive performance for class of extremely poor, we found that <code>naive_bayes</code> outperformed the other models in our current set-up.</p>
</div>
<div id="possible-next-steps" class="section level2">
<h2>Possible next steps</h2>
<p>As can be seen from the evaluation results, there is a lot of room for us to try to improve our predictive results. While that was not necessarily the aim of this particular post, for a future follow-up, there are many things we could do to try to improve upon our initial efforts here. For a future Part III to this series, we could look into:</p>
<ul>
<li>optimizing the best-performing model–this current analysis helped us identify some models that seemed to perform better than others. For a follow up, we could select some of these models, and then try to improve their performance specifically.</li>
<li>feature engineering–while we did some very basic pre-processing of our data to reduce its dimensionality, we did not try to create new features or integrate data from other sources to help improve our model.</li>
<li>optimize <span class="math inline">\(F_\beta\)</span>-score <span class="math inline">\(\beta\)</span> values–For our evaluation metrics, we just looked at three <span class="math inline">\(\beta\)</span> values, but <span class="math inline">\(\beta\)</span> can take on values up to <span class="math inline">\(\infty\)</span>. In the future, we could try to optimize this value and see how it affects performance of our classifier for both the least and most represented classes.</li>
<li>upsampling/downsampling–we could also try other techniques to address our class imbalance issues such as upsampling and downsampling.</li>
<li>binary classification–in fact, if we are most interested in identifying the most vulnerable populations are, we could simply transform the <code>Target</code> label in our dataset and turn this problem into a binary classification challenge. This would potentially simplify what we need to do allow us to build higher performing (more accurate) predictive models.</li>
</ul>
</div>
</div>
